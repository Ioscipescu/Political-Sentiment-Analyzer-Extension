{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python3 -m venv venv\n",
    "!source venv/bin/activate\n",
    "!pip install tensorflow transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 05:21:52.883370: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-05 05:21:53.169379: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-05 05:21:53.178488: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-05 05:21:53.178520: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-05 05:21:55.128659: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-05 05:21:55.128796: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-05 05:21:55.128811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/ben/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/BrettWisniewski/longdata/main/ThinkThisWillWorkTS.tsv?token=GHSAT0AAAAAAB6ODE7CMPU4X6A3335D2TF2ZAERZNA', encoding='cp1252', delimiter='\\t', header=None)\n",
    "with open('df.pkl', 'wb') as f:\n",
    "    pickle.dump((df), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['text', 'label']\n",
    "x = df.text.tolist()\n",
    "y = df.label.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: 'The transgender effort to suppress any recognition that men and women are different and complementary would not matter except for the movement’s political alliance with wealthy progressives and radical feminists who wish to destroy the political power of the male-and-female family.'\n",
      "input ids: [101, 1996, 16824, 3947, 2000, 16081, 2151, 5038, 2008, 2273, 1998, 2308, 2024, 2367, 1998, 21053, 2052, 2025, 3043, 3272, 2005, 1996, 2929, 1521, 1055, 2576, 4707, 2007, 7272, 6555, 2015, 1998, 7490, 10469, 2015, 2040, 4299, 2000, 6033, 1996, 2576, 2373, 1997, 1996, 3287, 1011, 1998, 1011, 2931, 2155, 1012, 102]\n",
      "attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LEN = 120\n",
    "\n",
    "review = x[0]\n",
    "\n",
    "tkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "inputs = tkzr(review, max_length=MAX_LEN, truncation=True, padding=True)\n",
    "\n",
    "print(f'review: \\'{review}\\'')\n",
    "print(f'input ids: {inputs[\"input_ids\"]}')\n",
    "print(f'attention mask: {inputs[\"attention_mask\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):\n",
    "    return tkzr(x, max_length=max_len, truncation=trucation, padding=padding)\n",
    "    \n",
    "encodings = construct_encodings(x, tkzr, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 05:22:31.559540: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-05 05:22:31.559576: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-05 05:22:31.559605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (catdog): /proc/driver/nvidia/version does not exist\n",
      "2023-03-05 05:22:31.560096: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    \n",
    "tfdataset = construct_tfdataset(encodings, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_size = int(len(x) * (1-TEST_SPLIT))\n",
    "\n",
    "tfdataset = tfdataset.shuffle(len(x))\n",
    "tfdataset_train = tfdataset.take(train_size)\n",
    "tfdataset_test = tfdataset.skip(train_size)\n",
    "\n",
    "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n",
    "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"tf_model.h5\";: 100%|██████████| 363M/363M [00:24<00:00, 15.1MB/s] \n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_projector', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From /home/ben/.local/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "5790/5790 [==============================] - 5083s 873ms/step - loss: 0.2002 - accuracy: 0.9051\n",
      "Epoch 2/2\n",
      "5790/5790 [==============================] - 5089s 879ms/step - loss: 0.0154 - accuracy: 0.9958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff7d6f4cb50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "optimizer = optimizers.Adam(learning_rate=3e-5)\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448/1448 [==============================] - 335s 229ms/step - loss: 0.0062 - accuracy: 0.9976\n",
      "{'loss': 0.006220742594450712, 'accuracy': 0.9975828528404236}\n"
     ]
    }
   ],
   "source": [
    "benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictor(model, model_name, max_len):\n",
    "  tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
    "  def predict_proba(text):\n",
    "      x = [text]\n",
    "\n",
    "      encodings = construct_encodings(x, tkzr, max_len=max_len)\n",
    "      tfdataset = construct_tfdataset(encodings)\n",
    "      tfdataset = tfdataset.batch(1)\n",
    "\n",
    "      preds = model.predict(tfdataset).logits\n",
    "      preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "      return preds[0][0]\n",
    "    \n",
    "  return predict_proba\n",
    "\n",
    "# clf = create_predictor(model, MODEL_NAME, MAX_LEN)\n",
    "# print(clf('Small government is good government.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('~/Document/clf')\n",
    "with open('info.pkl', 'wb') as f:\n",
    "    pickle.dump((MODEL_NAME, MAX_LEN), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at ~/Document/clf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ben/.local/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "0.9993524\n"
     ]
    }
   ],
   "source": [
    "new_model = TFDistilBertForSequenceClassification.from_pretrained('~/Document/clf')\n",
    "model_name, max_len = pickle.load(open('info.pkl', 'rb'))\n",
    "\n",
    "clf = create_predictor(new_model, model_name, max_len)\n",
    "print(clf('Small government is good government'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ~/Document/clf were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ~/Document/clf and are newly initialized: ['dropout_79']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float32' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m clf \u001b[39m=\u001b[39m create_predictor(model, \u001b[39m'\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m120\u001b[39m)\n\u001b[1;32m      6\u001b[0m outputs \u001b[39m=\u001b[39m clf(\u001b[39m\"\u001b[39m\u001b[39mSmall government is good government.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39;49mloss\n\u001b[1;32m      8\u001b[0m prediction_logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float32' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('~/Document/clf')\n",
    "\n",
    "clf = create_predictor(model, 'distilbert-base-uncased', 120)\n",
    "\n",
    "outputs = clf(\"Small government is good government.\")\n",
    "loss = outputs.loss\n",
    "prediction_logits = outputs.logits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
